{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'ПИ19-1'\n",
    "s2 = 'ПИ19-1'\n",
    "edit_distance(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# with open('../09_string_2/data/litw-win.txt') as fp:\n",
    "#     for line in fp:\n",
    "#         words.append(line.strip().split()[-1])\n",
    "        \n",
    "# words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'велечайшим'\n",
    "# min(words, key=lambda k: edit_distance(word,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "stemmer.stem('попрелагающимся')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mor = pymorphy2.MorphAnalyzer()\n",
    "mor.parse('попрелагающимся')[0].normalized.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'''\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_cv = cv.transform(sent).toarray()\n",
    "sent_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love is in the air  beef fondue   sauces</td>\n",
       "      <td>i think a fondue is a very romantic casual din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open sesame  noodles</td>\n",
       "      <td>this is a very versatile and widely enjoyed pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say what   banana sandwich</td>\n",
       "      <td>you just have to try it to believe it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 in canada chocolate chip cookies</td>\n",
       "      <td>this is the recipe that we use at my school ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>412 broccoli casserole</td>\n",
       "      <td>since there are already 411 recipes for brocco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18805</th>\n",
       "      <td>zucchini cheddar casserole</td>\n",
       "      <td>this has been a long time family favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18806</th>\n",
       "      <td>zucchini  courgette soup  good for weight watc...</td>\n",
       "      <td>this is a favourite winter warmer by british c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18807</th>\n",
       "      <td>zuppa by luisa</td>\n",
       "      <td>this soup is a hearty meal  from luisa musso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18808</th>\n",
       "      <td>zurie s holey rustic olive and cheddar bread</td>\n",
       "      <td>this is based on a french recipe but i changed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18809</th>\n",
       "      <td>cookies by design   cookies on a stick</td>\n",
       "      <td>ive heard of the cookies by design company but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18810 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0               love is in the air  beef fondue   sauces   \n",
       "1                                   open sesame  noodles   \n",
       "2                             say what   banana sandwich   \n",
       "3                     1 in canada chocolate chip cookies   \n",
       "4                                 412 broccoli casserole   \n",
       "...                                                  ...   \n",
       "18805                         zucchini cheddar casserole   \n",
       "18806  zucchini  courgette soup  good for weight watc...   \n",
       "18807                                     zuppa by luisa   \n",
       "18808       zurie s holey rustic olive and cheddar bread   \n",
       "18809             cookies by design   cookies on a stick   \n",
       "\n",
       "                                preprocessed_description  \n",
       "0      i think a fondue is a very romantic casual din...  \n",
       "1      this is a very versatile and widely enjoyed pa...  \n",
       "2                  you just have to try it to believe it  \n",
       "3      this is the recipe that we use at my school ca...  \n",
       "4      since there are already 411 recipes for brocco...  \n",
       "...                                                  ...  \n",
       "18805          this has been a long time family favorite  \n",
       "18806  this is a favourite winter warmer by british c...  \n",
       "18807       this soup is a hearty meal  from luisa musso  \n",
       "18808  this is based on a french recipe but i changed...  \n",
       "18809  ive heard of the cookies by design company but...  \n",
       "\n",
       "[18810 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_descript = pd.read_csv('../08_string/result/preprocessed_descriptions.csv', sep=',', index_col=0)\n",
    "p_descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpa',\n",
       " 'moistdense',\n",
       " 'memories',\n",
       " 'tomatoeshave',\n",
       " 'complimentsca',\n",
       " 'twolayer',\n",
       " 'pearcider',\n",
       " 'impart',\n",
       " 'relation',\n",
       " 'chin',\n",
       " 'burnin',\n",
       " 'california',\n",
       " 'liquidy',\n",
       " 'soupier',\n",
       " 'borders',\n",
       " 'tomatoesin',\n",
       " 'deborah',\n",
       " 'barley',\n",
       " 'failed',\n",
       " 'opor',\n",
       " 'kits',\n",
       " 'ancho',\n",
       " 'copy',\n",
       " 'nonwheat',\n",
       " 'makeshift',\n",
       " 'oamc',\n",
       " 'daily',\n",
       " 'circles',\n",
       " 'participation',\n",
       " 'binder',\n",
       " 'viennese',\n",
       " 'aperfectly',\n",
       " 'tendersjust',\n",
       " 'manjarblanco',\n",
       " 'recipes4duhmmies',\n",
       " 'halmonie',\n",
       " '2pound',\n",
       " 'workable',\n",
       " 'thickened',\n",
       " 'winemmm',\n",
       " 'concepts',\n",
       " 'thaws',\n",
       " 'wonderfulfully',\n",
       " 'vines',\n",
       " 'appetiser',\n",
       " 'baklava',\n",
       " 'microwaving',\n",
       " 'feud',\n",
       " 'dental',\n",
       " 'attractiveness',\n",
       " 'idiots',\n",
       " 'bunch',\n",
       " 'secondclass',\n",
       " 'vietnamesethai',\n",
       " 'pouches',\n",
       " 'hamtramck',\n",
       " 'pack',\n",
       " 'newspaper',\n",
       " 'addin',\n",
       " 'cinnabon',\n",
       " 'ventilated',\n",
       " 'heatadd',\n",
       " 'away',\n",
       " 'cantaloupe',\n",
       " 'crisped',\n",
       " 'standin',\n",
       " 'peasy',\n",
       " 'behind',\n",
       " 'restricted',\n",
       " 'itguaranteed',\n",
       " 'guesstimates',\n",
       " 'claimed',\n",
       " 'etc',\n",
       " 'gruyre',\n",
       " 'swaps',\n",
       " 'outthe',\n",
       " 'galestro',\n",
       " 'enjoymakes',\n",
       " 'haedrich',\n",
       " 'favourites',\n",
       " 'itnote',\n",
       " 'omelets',\n",
       " 'ears',\n",
       " 'serving',\n",
       " 'absurdly',\n",
       " 'elvee',\n",
       " 'whitelaw',\n",
       " 'junina',\n",
       " 'asked',\n",
       " 'casserolemy',\n",
       " 'schedule',\n",
       " 'preferrecipe',\n",
       " 'thickens',\n",
       " 'binge',\n",
       " '43473',\n",
       " 'octoberfest',\n",
       " 'bestwe',\n",
       " 'tahini',\n",
       " 'constructed',\n",
       " 'hock',\n",
       " 'limes',\n",
       " '1216',\n",
       " 'encourage',\n",
       " 'chilie',\n",
       " 'microwave',\n",
       " 'improving',\n",
       " 'producer',\n",
       " 'puto',\n",
       " 'through',\n",
       " 'nciely',\n",
       " 'nava',\n",
       " 'cookiesflavored',\n",
       " 'vegetablei',\n",
       " 'changes',\n",
       " 'apron',\n",
       " 'neverfail',\n",
       " 'peel',\n",
       " 'meatier',\n",
       " 'rachaels',\n",
       " 'reward',\n",
       " 'gosh',\n",
       " 'marbling',\n",
       " 'vicki',\n",
       " 'toothpick',\n",
       " '5',\n",
       " 'birings',\n",
       " 'amongst',\n",
       " 'gillespies',\n",
       " 'ls',\n",
       " 'contemporary',\n",
       " 'thawed',\n",
       " 'sauerbraten',\n",
       " 'clearing',\n",
       " 'sorbet',\n",
       " '45',\n",
       " 'chopsyou',\n",
       " '2009note',\n",
       " 'herbalists',\n",
       " 'popped',\n",
       " 'hubbyapproved',\n",
       " 'southerncajun',\n",
       " 'crawfish',\n",
       " 'nut',\n",
       " 'relate',\n",
       " 'unoaked',\n",
       " 'publication',\n",
       " 'buns',\n",
       " 'bean',\n",
       " 'thanks',\n",
       " 'week',\n",
       " 'choppera',\n",
       " 'occupies',\n",
       " '10inch',\n",
       " 'behaveill',\n",
       " 'halls',\n",
       " 'mandarins',\n",
       " 'bhg',\n",
       " 'habaneros',\n",
       " 'fefers',\n",
       " 'variable',\n",
       " 'cachet',\n",
       " 'httpwwwbbcgoodfoodcomrecipes9099lancashirehotpot',\n",
       " 'yesif',\n",
       " 'dear',\n",
       " 'true',\n",
       " 'log',\n",
       " 'sparks',\n",
       " 'ack',\n",
       " 'sunchokes',\n",
       " 'garbanzos',\n",
       " 'irrepressibly',\n",
       " 'seals',\n",
       " 'sunkist',\n",
       " 'wrights',\n",
       " 'recipezaarcom',\n",
       " 'removable',\n",
       " 'jersey',\n",
       " 'caloriefat',\n",
       " 'mealthese',\n",
       " '339838',\n",
       " 'stitts',\n",
       " 'buffetts',\n",
       " 'golean',\n",
       " 'address',\n",
       " 'newbies',\n",
       " 'halfed',\n",
       " 'mercenaries',\n",
       " 'giada',\n",
       " 'detail',\n",
       " 'reminding',\n",
       " '70',\n",
       " 'trivia',\n",
       " 'nepal',\n",
       " 'scots',\n",
       " 'remodeled',\n",
       " 'transcribing',\n",
       " 'caloriesthe',\n",
       " 'whoohoo',\n",
       " 'amarillo',\n",
       " 'sos',\n",
       " 'glorious',\n",
       " 'proteinsor',\n",
       " 'downs',\n",
       " 'rehydrate',\n",
       " 'vin',\n",
       " 'emealz',\n",
       " 'poundbundt',\n",
       " 'activenote',\n",
       " 'supestars',\n",
       " 'ford',\n",
       " 'dairyfree',\n",
       " 'tarts',\n",
       " 'aloopsies',\n",
       " 'ouimet',\n",
       " 'bundled',\n",
       " 'signifies',\n",
       " 'costamolino',\n",
       " 'hmm',\n",
       " 'partyi',\n",
       " 'globs',\n",
       " 'matzos',\n",
       " 'greece',\n",
       " 'dog',\n",
       " '12qt',\n",
       " 'wwwhelpforibscom',\n",
       " 'successfry',\n",
       " 'channel',\n",
       " 'melting',\n",
       " 'ellerbee',\n",
       " 'hearty',\n",
       " 'omeletplease',\n",
       " 'dividing',\n",
       " 'blackie',\n",
       " 'fatfree',\n",
       " 'casual',\n",
       " 'coffeecakes',\n",
       " 'chewie',\n",
       " 'eagle',\n",
       " 'banger',\n",
       " 'behalf',\n",
       " 'ri',\n",
       " 'emphasize',\n",
       " 'someone',\n",
       " 'naomi',\n",
       " 'darker',\n",
       " 'emile',\n",
       " 'kossman',\n",
       " 'cuts',\n",
       " 'pats',\n",
       " 'antioxidant',\n",
       " 'momstobe',\n",
       " 'valid',\n",
       " 'aluminium',\n",
       " 'stengel',\n",
       " 'wickedly',\n",
       " 'aroma',\n",
       " 'useless',\n",
       " 'em',\n",
       " 'norm',\n",
       " 'sriracha',\n",
       " 'uncook',\n",
       " 'varities',\n",
       " 'jaffa',\n",
       " 'resurgence',\n",
       " 'lukins',\n",
       " 'gives',\n",
       " 'carrots',\n",
       " 'relief',\n",
       " 'chocolatechocolatecovered',\n",
       " 'fromgermancookbookscom',\n",
       " 'youve',\n",
       " 'chafing',\n",
       " 'avoid',\n",
       " 'irishness',\n",
       " 'buttercake',\n",
       " 'kobayashis',\n",
       " 'generationsneed',\n",
       " 'gentleman',\n",
       " 'kneidlach',\n",
       " 'noodlesand',\n",
       " 'skewers',\n",
       " 'timeintensive',\n",
       " 'substitions',\n",
       " 'query',\n",
       " 'travelsafe',\n",
       " 'carcus',\n",
       " 'grillingbroiling',\n",
       " 'takei',\n",
       " 'lager',\n",
       " 'smoked',\n",
       " 'helped',\n",
       " 'alden',\n",
       " 'httphellyeahitsvegancomp755',\n",
       " 'allrecipes',\n",
       " 'maundy',\n",
       " 'bulgar',\n",
       " 'makeour',\n",
       " 'dropstyle',\n",
       " 'chaching',\n",
       " 'caterpillar',\n",
       " 'karo',\n",
       " 'prefect',\n",
       " 'chanuka',\n",
       " 'continue',\n",
       " 'wedge',\n",
       " 'bottledfrom',\n",
       " 'oooh',\n",
       " 'debbwl',\n",
       " 'pasanda',\n",
       " 'disappointed',\n",
       " 'gray',\n",
       " 'exclusions',\n",
       " 'wiltoncom',\n",
       " 'vibrance',\n",
       " 'bailiff',\n",
       " 'granulated',\n",
       " 'thinkposted',\n",
       " 'lemonsyou',\n",
       " 'catalog',\n",
       " 'lynnerossetto',\n",
       " 'everana',\n",
       " 'was',\n",
       " 'vindicated',\n",
       " 'pefect',\n",
       " 'fairy',\n",
       " 'sugeestions',\n",
       " 'jellyavailable',\n",
       " 'katie',\n",
       " 'prevention',\n",
       " 'harmonious',\n",
       " 'delays',\n",
       " 'morse',\n",
       " 'lexington',\n",
       " 'cinnamon',\n",
       " 'matched',\n",
       " 'ms',\n",
       " 'crustless',\n",
       " 'sacrificing',\n",
       " 'meadows',\n",
       " 'folded',\n",
       " 'caloriescarbs',\n",
       " 'clove',\n",
       " 'didnt',\n",
       " 'originates',\n",
       " '7layer',\n",
       " 'mentions',\n",
       " 'electronic',\n",
       " 'vinegars',\n",
       " 'ag',\n",
       " 'experiment',\n",
       " 'violas',\n",
       " 'smiling',\n",
       " 'wilson',\n",
       " 'souped',\n",
       " 'older',\n",
       " 'beta',\n",
       " '4s',\n",
       " 'hors',\n",
       " 'excellen',\n",
       " 'veryhighfat',\n",
       " 'wakarusa',\n",
       " 'net',\n",
       " 'velvety',\n",
       " 'themselveshttpwwwelanaspantrycomwhippedcreamfrosting',\n",
       " 'sucking',\n",
       " 'theseand',\n",
       " 'cheeseif',\n",
       " 'mignons',\n",
       " 'hardly',\n",
       " 'arab',\n",
       " 'tightfitting',\n",
       " 'downtown',\n",
       " 'startedstill',\n",
       " 'shipyard',\n",
       " 'sweetener',\n",
       " 'lasagnamaking',\n",
       " 'thatched',\n",
       " 'pureed',\n",
       " 'verts',\n",
       " 'whenever',\n",
       " 'ease',\n",
       " 'deciphered',\n",
       " '6cup',\n",
       " 'neglected',\n",
       " 'returns',\n",
       " 'applied',\n",
       " 'correcting',\n",
       " 'pompeis',\n",
       " 'gaapril',\n",
       " 'pf',\n",
       " 'wellused',\n",
       " 'onerise',\n",
       " 'subhoagiehero',\n",
       " 'kids',\n",
       " 'rhurbarb',\n",
       " 'boned',\n",
       " 'thx',\n",
       " '123s',\n",
       " '17g',\n",
       " 'australias',\n",
       " 'cultured',\n",
       " 'menopause',\n",
       " 'maclarens',\n",
       " 'cause',\n",
       " 'cornwall',\n",
       " 'weeklys',\n",
       " 'glasses',\n",
       " 'dr',\n",
       " 'convenience',\n",
       " 'typical',\n",
       " 'lately',\n",
       " 'fantasticsounding',\n",
       " 'refuse',\n",
       " 'heck',\n",
       " 'id',\n",
       " 'livingposting',\n",
       " 'taffy',\n",
       " 'propane',\n",
       " 'sealing',\n",
       " 'formed',\n",
       " 'cinco',\n",
       " 'failures',\n",
       " 'reismans',\n",
       " 'fireplace',\n",
       " 'philippines',\n",
       " 'involves',\n",
       " 'likeim',\n",
       " 'lunchesbreakfast',\n",
       " 'foodmeatloaf',\n",
       " 'looked',\n",
       " 'chokel',\n",
       " 'rollsadd',\n",
       " 'favouritesthe',\n",
       " 'pizazz',\n",
       " 'wrongnote',\n",
       " 'smokies',\n",
       " 'knots',\n",
       " 'favoritehe',\n",
       " 'desired',\n",
       " '11th',\n",
       " 'teething',\n",
       " 'heathy',\n",
       " 'medditeranean',\n",
       " '512',\n",
       " 'deprive',\n",
       " 'dalmatian',\n",
       " 'pinkish',\n",
       " 'doughnuts',\n",
       " 'snappers',\n",
       " 'scary',\n",
       " 'rare',\n",
       " 'trinigourmetcom',\n",
       " 'bchamel',\n",
       " 'screams',\n",
       " 'bitchin',\n",
       " 'lucy',\n",
       " 'dishreally',\n",
       " 'joined',\n",
       " 'caravans',\n",
       " 'knowstrange',\n",
       " 'recommends',\n",
       " 'sigh',\n",
       " 'beautifully',\n",
       " 'differentegg',\n",
       " 'guar',\n",
       " 'viewings',\n",
       " 'fingerlickin',\n",
       " 'fifties',\n",
       " 'subject',\n",
       " 'muffuletta',\n",
       " 'redneck',\n",
       " 'virgin',\n",
       " 'zatarins',\n",
       " 'specifications',\n",
       " 'loureno',\n",
       " 'tothe',\n",
       " 'raita',\n",
       " 'variant',\n",
       " 'harshness',\n",
       " 'chronic',\n",
       " '80739',\n",
       " 'zurich',\n",
       " 'parens',\n",
       " 'hav',\n",
       " 'dinner',\n",
       " 'wisconsin',\n",
       " 'hootie',\n",
       " 'elegance',\n",
       " 'biller',\n",
       " 'boullion',\n",
       " 'encrusted',\n",
       " 'rockfish',\n",
       " 'pairingswine',\n",
       " 'preparationcooking',\n",
       " 'compete',\n",
       " 'potpie',\n",
       " 'fluke',\n",
       " 'goodwell',\n",
       " 'wharf',\n",
       " 'jalisco',\n",
       " 'chickencan',\n",
       " 'pleaser',\n",
       " 'wraps',\n",
       " 'superquick',\n",
       " 'room',\n",
       " 'shrimpthe',\n",
       " 'likeupdate',\n",
       " 'endangered',\n",
       " 'bowies',\n",
       " 'authenticity',\n",
       " 'pastas',\n",
       " 'raisinssuperb',\n",
       " 'dieting',\n",
       " 'isles',\n",
       " 'jones',\n",
       " 'michal',\n",
       " 'doubling',\n",
       " 'skewersand',\n",
       " 'charcoalwood',\n",
       " 'hmmm',\n",
       " 'succotash',\n",
       " 'moghul',\n",
       " 'gelsons',\n",
       " 'crisper',\n",
       " 'cajuns',\n",
       " 'recipe71644',\n",
       " 'snapper',\n",
       " 'veganfoodnet',\n",
       " 'hiking',\n",
       " 'fla',\n",
       " 'fiona',\n",
       " 'frypan',\n",
       " 'skewer',\n",
       " 'thickcut',\n",
       " 'torito',\n",
       " 'emerilscom',\n",
       " 'attention',\n",
       " 'mutedorange',\n",
       " 'similar',\n",
       " 'variables',\n",
       " 'enoughseason',\n",
       " 'licence',\n",
       " 'mud',\n",
       " 'bluegreen',\n",
       " 'entendres',\n",
       " 'countrymomcom',\n",
       " 'apatitehere',\n",
       " 'greenlawn',\n",
       " 'easyfrenchfoodcom',\n",
       " 'napalm',\n",
       " 'utensils',\n",
       " '112708',\n",
       " '61507',\n",
       " 'cloths',\n",
       " 'stable',\n",
       " 'giving',\n",
       " 'pushcarts',\n",
       " 'sugar15',\n",
       " 'preminced',\n",
       " 'government',\n",
       " 'habnero',\n",
       " 'prescribed',\n",
       " 'ragati',\n",
       " 'timings',\n",
       " 'recipe311449',\n",
       " 'thoroughly',\n",
       " 'waiting',\n",
       " 'chiarellos',\n",
       " 'geordie',\n",
       " 'crockpotif',\n",
       " 'glossy',\n",
       " 'jaime',\n",
       " 'cruch',\n",
       " 'anselmi',\n",
       " 'dennis',\n",
       " 'blended',\n",
       " 'didja',\n",
       " 'kidsfor',\n",
       " 'impress',\n",
       " 'approximationyou',\n",
       " 'fajita',\n",
       " 'wonderfulit',\n",
       " 'teddys',\n",
       " 'browns',\n",
       " 'telling',\n",
       " 'shivraj',\n",
       " 'showtime',\n",
       " 'commanders',\n",
       " 'bisquit',\n",
       " 'manly',\n",
       " 'misonutritional',\n",
       " 'b15',\n",
       " 'frenchcanadian',\n",
       " 'conestessa',\n",
       " 'baja',\n",
       " 'ovenready',\n",
       " 'herhehe',\n",
       " 'driving',\n",
       " 'eatingwellcom',\n",
       " 'sodiumand',\n",
       " 'lettucepickles',\n",
       " 'upgreat',\n",
       " 'shallot',\n",
       " 'groups',\n",
       " 'rajmah',\n",
       " 'liana',\n",
       " 'doyou',\n",
       " 'already',\n",
       " 'neglect',\n",
       " 'kindssmooth',\n",
       " 'varion',\n",
       " 'garnet',\n",
       " 'gng',\n",
       " 'calmex',\n",
       " 'cardiac',\n",
       " 'pitcher',\n",
       " 'chelow',\n",
       " '623',\n",
       " 'gi',\n",
       " 'nice',\n",
       " 'loan',\n",
       " 'awhile',\n",
       " 'gop',\n",
       " 'harlem',\n",
       " 'enhanced',\n",
       " 'fitzer',\n",
       " 'cranberriesi',\n",
       " 'whittling',\n",
       " 'patrickgoudreau',\n",
       " 'stirfrys',\n",
       " 'factor',\n",
       " 'form',\n",
       " 'collapses',\n",
       " 'magdaleen',\n",
       " 'kulski',\n",
       " 'mitsitam',\n",
       " 'rebro',\n",
       " 'hung',\n",
       " 'cece',\n",
       " 'void',\n",
       " 'jiffy',\n",
       " 'witty',\n",
       " 'orlando',\n",
       " 'casings',\n",
       " 'lemonade',\n",
       " 'quintessential',\n",
       " 'kathryn',\n",
       " 'nannie',\n",
       " 'endless',\n",
       " 'beverage',\n",
       " 'manages',\n",
       " 'irene',\n",
       " 'kalua',\n",
       " 'amout',\n",
       " 'lovefor',\n",
       " 'hbo',\n",
       " 'ciders',\n",
       " 'similiar',\n",
       " 'cobbler',\n",
       " 'curtain',\n",
       " 'familiarity',\n",
       " 'shampoo',\n",
       " '75yearold',\n",
       " 'lined',\n",
       " 'deliciousthe',\n",
       " 'mixedup',\n",
       " 'disappear',\n",
       " 'ups',\n",
       " 'forgetit',\n",
       " 'tails',\n",
       " 'itmade',\n",
       " 'fancyschmancy',\n",
       " 'variationi',\n",
       " 'perfer',\n",
       " 'garage',\n",
       " 'surplus',\n",
       " 'labour',\n",
       " 'boureki',\n",
       " 'grease',\n",
       " 'storecupboard',\n",
       " 'juices',\n",
       " 'hadthe',\n",
       " 'skip',\n",
       " 'blue',\n",
       " 'gwen',\n",
       " 'lambmutton',\n",
       " 'balance',\n",
       " 'distributors',\n",
       " 'thatfor',\n",
       " 'jacques',\n",
       " 'weiners',\n",
       " 'gramflour',\n",
       " 'kburies',\n",
       " 'friendlyhttpwwwgenawcomlowcarbmexicanricehtml',\n",
       " 'scrub',\n",
       " '19th',\n",
       " 'katona',\n",
       " 'huevos',\n",
       " 'elim',\n",
       " 'americacaribbean',\n",
       " 'byerlys',\n",
       " 'chapter',\n",
       " 'httpwwwfatfreevegancom',\n",
       " 'fairly',\n",
       " '63g',\n",
       " 'breadif',\n",
       " 'lil',\n",
       " 'boomettes',\n",
       " 'grandmothernana',\n",
       " 'flanken',\n",
       " 'josep',\n",
       " 'burritos',\n",
       " 'waukesha',\n",
       " 'marsha',\n",
       " 'recipegoldmine',\n",
       " 'attractively',\n",
       " 'fascimile',\n",
       " 'longit',\n",
       " 'supremely',\n",
       " 'going',\n",
       " 'mixture',\n",
       " 'sidekick',\n",
       " 'russ',\n",
       " 'soupa',\n",
       " 'greatnephew',\n",
       " 'breakfastyummy',\n",
       " 'lowercalorie',\n",
       " 'htipiti',\n",
       " 'partook',\n",
       " 'rucker',\n",
       " 'flavornice',\n",
       " 'finland',\n",
       " 'triumph',\n",
       " 'health',\n",
       " 'essentially',\n",
       " '343g',\n",
       " 'perfected',\n",
       " 'silverewater',\n",
       " 'jalpenos',\n",
       " 'copied',\n",
       " 'harks',\n",
       " 'overdone',\n",
       " 'dishcooking',\n",
       " 'kewra',\n",
       " 'still',\n",
       " 'amberjack',\n",
       " 'flavorsrecipe',\n",
       " 'juicezest',\n",
       " 'whitneys',\n",
       " 'fiances',\n",
       " 'simliar',\n",
       " 'overlong',\n",
       " 'groundhave',\n",
       " 'chelseas',\n",
       " 'jerusalem',\n",
       " 'bbqed',\n",
       " 'ricotta',\n",
       " '332',\n",
       " 'sticktoyourribs',\n",
       " 'olde',\n",
       " 'calder',\n",
       " 'reminds',\n",
       " 'unconventional',\n",
       " 'extracts',\n",
       " 'asian',\n",
       " 'tempura',\n",
       " 'rululach',\n",
       " 'cathi',\n",
       " 'devour',\n",
       " 'monks',\n",
       " 'foodwell',\n",
       " 'offwhite',\n",
       " 'variationyou',\n",
       " 'chao',\n",
       " 'refirgerate',\n",
       " 'eachold',\n",
       " 'yeast',\n",
       " '35gmono',\n",
       " 'rightthis',\n",
       " 'practices',\n",
       " 'oventhey',\n",
       " 'soooo',\n",
       " 'elaine',\n",
       " 'temperance',\n",
       " 'grogan',\n",
       " 'kabsa',\n",
       " 'glitch',\n",
       " 'otheri',\n",
       " 'themthey',\n",
       " 'skinless',\n",
       " '77465',\n",
       " 'wont',\n",
       " 'staple',\n",
       " 'pimentostuffed',\n",
       " 'santorini',\n",
       " 'lowsugar',\n",
       " 'private',\n",
       " 'restaurantsclubs',\n",
       " 'greattasting',\n",
       " 'wholesome',\n",
       " 'portion',\n",
       " 'printed',\n",
       " 'shortcake',\n",
       " '80960',\n",
       " 'varieities',\n",
       " 'carniverous',\n",
       " 'nouba',\n",
       " 'crusted',\n",
       " 'ulms',\n",
       " 'bourbonwhats',\n",
       " 'peggy',\n",
       " 'leaf',\n",
       " '205074',\n",
       " 'fattier',\n",
       " 'signs',\n",
       " 'issues',\n",
       " 'klippfiskclipfish',\n",
       " 'loved',\n",
       " 'achieving',\n",
       " 'fondest',\n",
       " 'gyoza',\n",
       " 'costcoeasy',\n",
       " 'hangover',\n",
       " 'napkins',\n",
       " 'sageparmesan',\n",
       " 'quicktofix',\n",
       " 'vegemight',\n",
       " 'mclean',\n",
       " 'pickme',\n",
       " 'hundredth',\n",
       " 'outdoors',\n",
       " 'aired',\n",
       " 'oneprep',\n",
       " 'topic',\n",
       " 'husbands',\n",
       " 'tm',\n",
       " 'maku',\n",
       " 'perked',\n",
       " 'clams',\n",
       " 'handokay',\n",
       " 'wonderfull',\n",
       " 'mulled',\n",
       " 'bowels',\n",
       " 'urgehot',\n",
       " '83',\n",
       " 'pressed',\n",
       " 'onions',\n",
       " 'tryed',\n",
       " 'fernandez',\n",
       " 'trash',\n",
       " 'rebekah',\n",
       " 'malayindonesian',\n",
       " 'heart',\n",
       " 'cinnamonmy',\n",
       " 'restingi',\n",
       " 'usuallyrecipe',\n",
       " 'leathery',\n",
       " 'morel',\n",
       " 'preempt',\n",
       " 'chickenmushrooms',\n",
       " 'gingerroot',\n",
       " 'chewiest',\n",
       " 'chilebased',\n",
       " 'onboard',\n",
       " 'pdx',\n",
       " 'disposible',\n",
       " 'resultsyou',\n",
       " 'replacer',\n",
       " 'get',\n",
       " 'lunchessuppers',\n",
       " 'takeone',\n",
       " 'smart',\n",
       " 'troup',\n",
       " 'beefnote',\n",
       " 'frowned',\n",
       " 'overwork',\n",
       " 'lovethese',\n",
       " 'carrotsi',\n",
       " 'dropping',\n",
       " 'chunks',\n",
       " 'nation',\n",
       " 'recipepublished',\n",
       " 'colesium',\n",
       " 'ran',\n",
       " 'glut',\n",
       " 'foodstores',\n",
       " 'phoenix',\n",
       " '1941',\n",
       " 'df',\n",
       " 'volunteer',\n",
       " 'swap',\n",
       " 'currylike',\n",
       " 'malay',\n",
       " 'fatal',\n",
       " 'throw',\n",
       " 'nanny',\n",
       " 'differant',\n",
       " 'popular',\n",
       " 'coast',\n",
       " 'brendans',\n",
       " 'concern',\n",
       " 'hellsbackbonegrillcom',\n",
       " 'relationship',\n",
       " 'sanyo',\n",
       " 'held',\n",
       " 'dayi',\n",
       " '15th16th',\n",
       " 'herby',\n",
       " 'subbing',\n",
       " 'iii',\n",
       " 'marrakesh',\n",
       " 'metabolism',\n",
       " 'enjoyplease',\n",
       " 'vegasthe',\n",
       " 'scrum',\n",
       " 'refris',\n",
       " 'lunchboxes',\n",
       " 'purchases',\n",
       " 'bands',\n",
       " 'acknowledge',\n",
       " 'database',\n",
       " 'sweetestenjoy',\n",
       " 'containter',\n",
       " 'pastrywhich',\n",
       " 'purees',\n",
       " 'icingthe',\n",
       " 'storescooking',\n",
       " 'supply',\n",
       " 'jolokia',\n",
       " 'goudas',\n",
       " 'bozeman',\n",
       " 'each',\n",
       " 'morerecipe',\n",
       " 'cucina',\n",
       " 'cobblers',\n",
       " 'peelcan',\n",
       " 'extras',\n",
       " 'snag',\n",
       " '2nd',\n",
       " 'insalata',\n",
       " 'however',\n",
       " 'herbed',\n",
       " 'shape',\n",
       " 'yeasty',\n",
       " 'minfrom',\n",
       " 'chad',\n",
       " 'geri',\n",
       " 'cape',\n",
       " 'burned',\n",
       " 'munchies',\n",
       " 'coffee',\n",
       " 'squirrel',\n",
       " 'daz',\n",
       " 'successmemphis',\n",
       " 'budgetbytescom',\n",
       " 'list',\n",
       " 'eggsthis',\n",
       " 'dominos',\n",
       " 'omission',\n",
       " 'stephanie',\n",
       " 'yule',\n",
       " 'fathers',\n",
       " 'leasti',\n",
       " 'tricolor',\n",
       " 'dedicate',\n",
       " 'perk',\n",
       " 'rossipastacom',\n",
       " 'manhattan',\n",
       " 'splotched',\n",
       " 'nogales',\n",
       " 'agoa',\n",
       " 'fullon',\n",
       " 'dakotas',\n",
       " 'benefit',\n",
       " 'anchor',\n",
       " 'tattered',\n",
       " 'mings',\n",
       " 'yesits',\n",
       " 'pc',\n",
       " 'soggyby',\n",
       " 'tastefully',\n",
       " 'breadbowl',\n",
       " 'releases',\n",
       " 'otherwisebam',\n",
       " 'toffee',\n",
       " 'exactlyand',\n",
       " 'wwwbobsredmillcom',\n",
       " 'httpwwwrecipezaarcombbviewtopiczspt261947postdays0postorderascstart30',\n",
       " 'piece',\n",
       " 'vahterzoy',\n",
       " 'dumped',\n",
       " 'normans',\n",
       " 'samosas',\n",
       " 'rsvp',\n",
       " 'sitecoffee',\n",
       " 'drooling',\n",
       " 'rights',\n",
       " 'pantryfridge',\n",
       " 'hocking',\n",
       " 'thoughts',\n",
       " ...]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for row in p_descript['preprocessed_description']:\n",
    "    new_words = []\n",
    "    for i in nltk.word_tokenize(str(row)):\n",
    "        i = re.sub(\"[.,:!? ]\",\"\",i)\n",
    "        new_words.append(str(i))\n",
    "    words.extend(new_words)\n",
    "#     print(words)\n",
    "\n",
    "\n",
    "words = list(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.metrics.distance import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f12():\n",
    "    for i in range(5):\n",
    "        a = random.choice(words)\n",
    "        b = random.choice(words)\n",
    "        dist = edit_distance(a,b)\n",
    "        print(f\"Слово 1: {a}\\nСлово 2: {b}\\nРасстояние редактирования {dist}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово 1: 102854\n",
      "Слово 2: boxes\n",
      "Расстояние редактирования 6\n",
      "\n",
      "Слово 1: soupbroth\n",
      "Слово 2: adaptions\n",
      "Расстояние редактирования 7\n",
      "\n",
      "Слово 1: joes\n",
      "Слово 2: chaat\n",
      "Расстояние редактирования 5\n",
      "\n",
      "Слово 1: man\n",
      "Слово 2: feb2006\n",
      "Расстояние редактирования 7\n",
      "\n",
      "Слово 1: 509\n",
      "Слово 2: delicates\n",
      "Расстояние редактирования 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f12()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f13(word, k):\n",
    "    for i in range(len(words)):\n",
    "        if edit_distance(word, words[i]) == 0:\n",
    "            for j in range(k+1):\n",
    "                print(words[i+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "elecrtic\n",
      "stomach\n",
      "ptsorzo\n"
     ]
    }
   ],
   "source": [
    "f13('like', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yanalazareva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bpa</th>\n",
       "      <td>bpa</td>\n",
       "      <td>bpa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moistdense</th>\n",
       "      <td>moistdens</td>\n",
       "      <td>moistdense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>memori</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomatoeshave</th>\n",
       "      <td>tomatoeshav</td>\n",
       "      <td>tomatoeshave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complimentsca</th>\n",
       "      <td>complimentsca</td>\n",
       "      <td>complimentsca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longhornmama</th>\n",
       "      <td>longhornmama</td>\n",
       "      <td>longhornmama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burgandy</th>\n",
       "      <td>burgandi</td>\n",
       "      <td>burgandy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acheived</th>\n",
       "      <td>acheiv</td>\n",
       "      <td>acheived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavor</th>\n",
       "      <td>flavor</td>\n",
       "      <td>flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freshen</th>\n",
       "      <td>freshen</td>\n",
       "      <td>freshen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24825 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                stemmed_word normalized_word\n",
       "word                                        \n",
       "bpa                      bpa             bpa\n",
       "moistdense         moistdens      moistdense\n",
       "memories              memori          memory\n",
       "tomatoeshave     tomatoeshav    tomatoeshave\n",
       "complimentsca  complimentsca   complimentsca\n",
       "...                      ...             ...\n",
       "longhornmama    longhornmama    longhornmama\n",
       "burgandy            burgandi        burgandy\n",
       "acheived              acheiv        acheived\n",
       "flavor                flavor          flavor\n",
       "freshen              freshen         freshen\n",
       "\n",
       "[24825 rows x 2 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "st = []\n",
    "wn = []\n",
    "for i in words:\n",
    "    st.append(stm.stem(i))\n",
    "    wn.append(wnl.lemmatize(i))\n",
    "    \n",
    "pd_words = pd.DataFrame({ 'word' : words, 'stemmed_word' : st, 'normalized_word' : wn})\n",
    "pd_words = pd_words.set_index('word')\n",
    "pd_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from nltk.corpus import stopwords\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yanalazareva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-ea73915f3cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mwords1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mwords1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "words_before = []\n",
    "words_after = []\n",
    "words1 = copy.deepcopy(words)\n",
    "for i in range(len(words1)):\n",
    "    if words1[i].lower() in stopwords.words('english'):\n",
    "        try:\n",
    "            words1.remove(words1[i])\n",
    "            words_before.append(words1[i-1])\n",
    "            words_after.append(words1[i+1])\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_before = nltk.FreqDist(words_before)\n",
    "words_after = nltk.FreqDist(words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'nava': 1, 'pefect': 1, 'cucina': 1, 'proven': 1, 'iceberg': 1, 'venison': 1, 'broccolini': 1, 'round': 1, 'definantly': 1, 'highestquality': 1, ...})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['puto',\n",
       " 'everana',\n",
       " 'bozeman',\n",
       " 'poarch',\n",
       " 'whitewholewheatflour',\n",
       " 'fishers',\n",
       " 'range',\n",
       " 'whichever',\n",
       " 'crew',\n",
       " 'decor']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx for idx,val in words_before.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nava',\n",
       " 'pefect',\n",
       " 'cucina',\n",
       " 'proven',\n",
       " 'iceberg',\n",
       " 'venison',\n",
       " 'broccolini',\n",
       " 'round',\n",
       " 'definantly',\n",
       " 'highestquality']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx for idx,val in words_after.most_common(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 случайных рецептов\n",
    "p_descripts_sample = p_descript.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(p_descripts_sample.preprocessed_description).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 70)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вектор 1 -> \n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.30499223 0.30499223 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.30499223 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.24606581 0.\n",
      " 0.30499223 0.         0.         0.         0.         0.\n",
      " 0.         0.30499223 0.         0.30499223 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.30499223\n",
      " 0.         0.30499223 0.24606581 0.         0.         0.\n",
      " 0.20425685 0.         0.         0.         0.         0.30499223\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "вектор 2 -> \n",
      "[0.         0.         0.         0.33333333 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333333 0.         0.         0.         0.33333333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333333 0.         0.         0.         0.         0.\n",
      " 0.33333333 0.         0.         0.         0.33333333 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.33333333 0.33333333 0.33333333 0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "вектор 3 -> \n",
      "[0.42799292 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.63907044 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.63907044\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "вектор 4 -> \n",
      "[0.09404799 0.         0.42129172 0.         0.         0.1132985\n",
      " 0.         0.         0.         0.         0.         0.14043057\n",
      " 0.14043057 0.14043057 0.1132985  0.         0.         0.28086115\n",
      " 0.         0.14043057 0.         0.14043057 0.         0.14043057\n",
      " 0.         0.14043057 0.         0.14043057 0.14043057 0.1132985\n",
      " 0.14043057 0.28086115 0.         0.         0.         0.14043057\n",
      " 0.         0.         0.22659701 0.14043057 0.14043057 0.14043057\n",
      " 0.         0.         0.         0.         0.14043057 0.\n",
      " 0.         0.14043057 0.         0.14043057 0.         0.\n",
      " 0.14043057 0.         0.         0.22659701 0.1132985  0.28086115\n",
      " 0.09404799 0.14043057 0.         0.         0.         0.\n",
      " 0.14043057 0.         0.14043057 0.        ]\n",
      "\n",
      "вектор 5 -> \n",
      "[0.12145969 0.36272234 0.         0.         0.18136117 0.14632105\n",
      " 0.18136117 0.18136117 0.18136117 0.         0.         0.\n",
      " 0.         0.         0.14632105 0.18136117 0.18136117 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.36272234 0.         0.         0.2926421\n",
      " 0.         0.         0.18136117 0.18136117 0.14632105 0.\n",
      " 0.         0.18136117 0.14632105 0.         0.         0.\n",
      " 0.         0.         0.18136117 0.         0.         0.\n",
      " 0.         0.         0.18136117 0.         0.         0.\n",
      " 0.         0.         0.14632105 0.14632105 0.14632105 0.\n",
      " 0.24291938 0.         0.         0.         0.         0.\n",
      " 0.         0.18136117 0.         0.18136117]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(matrix.shape[0]):\n",
    "    print(f'вектор {i+1} -> \\n{matrix[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance = pd.DataFrame(p_descripts_sample.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_matrix = [[distance.cosine(matrix[i], matrix[j]) \n",
    "                      for j in range(len(cosine_distance.name))] \n",
    "                         for i in range(len(cosine_distance.name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cube steak skillet supper</th>\n",
       "      <th>breakfast cinnamon and apple squares</th>\n",
       "      <th>steak supper special</th>\n",
       "      <th>one rib for two</th>\n",
       "      <th>weetbix based chocolate slice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cube steak skillet supper</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980790</td>\n",
       "      <td>0.878373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast cinnamon and apple squares</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steak supper special</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959748</td>\n",
       "      <td>0.948016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one rib for two</th>\n",
       "      <td>0.980790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weetbix based chocolate slice</th>\n",
       "      <td>0.878373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948016</td>\n",
       "      <td>0.816529</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cube steak skillet supper  \\\n",
       "name                                                              \n",
       "cube steak skillet supper                              0.000000   \n",
       "breakfast cinnamon and apple squares                   1.000000   \n",
       "steak supper special                                   1.000000   \n",
       "one rib for two                                        0.980790   \n",
       "weetbix based chocolate slice                          0.878373   \n",
       "\n",
       "                                      breakfast cinnamon and apple squares  \\\n",
       "name                                                                         \n",
       "cube steak skillet supper                                              1.0   \n",
       "breakfast cinnamon and apple squares                                   0.0   \n",
       "steak supper special                                                   1.0   \n",
       "one rib for two                                                        1.0   \n",
       "weetbix based chocolate slice                                          1.0   \n",
       "\n",
       "                                      steak supper special  one rib for two  \\\n",
       "name                                                                          \n",
       "cube steak skillet supper                         1.000000         0.980790   \n",
       "breakfast cinnamon and apple squares              1.000000         1.000000   \n",
       "steak supper special                              0.000000         0.959748   \n",
       "one rib for two                                   0.959748         0.000000   \n",
       "weetbix based chocolate slice                     0.948016         0.816529   \n",
       "\n",
       "                                      weetbix based chocolate slice  \n",
       "name                                                                 \n",
       "cube steak skillet supper                                  0.878373  \n",
       "breakfast cinnamon and apple squares                       1.000000  \n",
       "steak supper special                                       0.948016  \n",
       "one rib for two                                            0.816529  \n",
       "weetbix based chocolate slice                              0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for name in cosine_distance.name:\n",
    "    cosine_distance[name] = cosine_matrix[count]\n",
    "    count += 1\n",
    "    \n",
    "cosine_distance.set_index('name', inplace=True)\n",
    "\n",
    "cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8165292749847445"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# меньше косинус = больше схожеть строк\n",
    "# но в таблице строка сравнивается сама с собой, поэтому убираем полное совпадение 0.0\n",
    "match = min([(cosine_distance[i].nsmallest(2)).max() for i in cosine_distance.columns])\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# значений 2, т.к. у нас совпадение и столбец-строка, и строка-столбец (зеркально относительно диагонали)\n",
    "i, j = np.where(cosine_distance.values == match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем, что значения идентичны и можем к ним обращаться\n",
    "cosine_distance.columns[j[::-1]] == cosine_distance.columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my kids dont like beef so i like making this for my husband and myself  we can have a nice roast beef dinner without leftovers its easy too prep time does not include the time it takes for the beef to freeze'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_descripts_sample.iloc[i[0]].preprocessed_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lovely easy chocolate slice that the kids love is cheap to make and can be modified to be white or brown choc flavored yummy this is my kids favorite'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_descripts_sample.iloc[j[0]].preprocessed_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итог: строки похожи на 1-0,8165 = 18% -> очень мало слов совпадает"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
